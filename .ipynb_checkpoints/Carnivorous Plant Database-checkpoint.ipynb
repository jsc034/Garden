{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Run this code to create a JSON file that diables auto-quotes and auto-brackets. After executing the Python \n",
    "#command, or manually creating the file, restart your Jupyter notebook, and it should stop auto-closing quotes\n",
    "#and brackets.\n",
    "\n",
    "#from notebook.services.config import ConfigManager\n",
    "#c = ConfigManager()\n",
    "#c.update('notebook', {\"CodeCell\": {\"cm_config\": {\"autoCloseBrackets\": False}}})\n",
    "\n",
    "###Keyboard Shortcuts\n",
    "#Ctrl + Enter: Run single cell of code (similar to R)\n",
    "#Ctrl + Shift + Enter: Run entire notebook (similar to R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intro to Web Scraping and BeautifulSoup\n",
    "#https://www.youtube.com/watch?v=XQgXKtPSzUI\n",
    "\n",
    "#Beautiful Soup HTTP Error 403\n",
    "\n",
    "\n",
    "#exporting to excel\n",
    "#df.to_excel(\"excel_file_name.xlsx\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "#import re #for splitting strings using multiple delimiters\n",
    "import time\n",
    "import random\n",
    "\n",
    "names = []\n",
    "prices = []\n",
    "saleprices = []\n",
    "soldouts = []\n",
    "stores = []\n",
    "species = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When populating species_XXX variables, make sure that they are all lowercase; to make comparison easier\n",
    "#Common name on left side, taxonomy on right\n",
    "species_vft = ['venus','flytrap','flytraps','dionaea','muscipula']\n",
    "species_neps = ['tropical','asian','nepenthes']\n",
    "species_sarracenia = ['american','sarracenia']\n",
    "species_sundew = ['sundew','sundews','drosera']\n",
    "species_utric = ['bladderwort','bladderworts','utricularia']\n",
    "species_pings = ['butterwort','butterworts','pinguicula']\n",
    "species_cephs = ['australian','cephalotus']\n",
    "species_darlingtonia = ['cobra','lily','lilies','darlingtonia']\n",
    "species_heli = ['sun','heliamphora']\n",
    "#species_special = ['specimen plants','carnivero exclusives']\n",
    "\n",
    "species_all = species_vft + species_neps + species_sarracenia + species_sundew + species_utric \\\n",
    "            + species_pings + species_cephs + species_darlingtonia + species_heli\n",
    "    \n",
    "#function to check type of species\n",
    "#name is the name of the item as a string, returns a string of the type of species\n",
    "def check_species(name):\n",
    "    #cleaning: replacing special characters with spaces, all lowercase then splitting words into list\n",
    "    kind_list = name.strip().replace('(','').replace(')','').replace('[','').replace(']','').replace(',','')\n",
    "    kind_list = kind_list.lower().split(' ')\n",
    "    \n",
    "    n = len(kind_list)\n",
    "    if n == 0:\n",
    "        print('error: length of kind is 0, cannot loop')\n",
    "        return 'error'\n",
    "    \n",
    "    #will try each element in the kind list and see if it is in species_XXX\n",
    "    #if not, will keep looping over the length of n\n",
    "    for i in range(n):\n",
    "        if kind_list[i] in species_vft:\n",
    "            return 'dionaea muscipula'\n",
    "        elif kind_list[i] in species_neps:\n",
    "            return 'nepenthes'\n",
    "        elif kind_list[i] in species_sarracenia:\n",
    "            return 'sarracenia'    \n",
    "        elif kind_list[i] in species_sundew:\n",
    "            return 'drosera'\n",
    "        elif kind_list[i] in species_pings:\n",
    "            return 'pinguicula'\n",
    "        elif kind_list[i] in species_utric:\n",
    "            return 'utricularia'\n",
    "        elif kind_list[i] in species_cephs:\n",
    "            return 'cephalotus'\n",
    "        elif kind_list[i] in species_darlingtonia:\n",
    "            return 'darlingtonia'\n",
    "        elif kind_list[i] in species_heli:\n",
    "            return 'heliamphora'\n",
    "        elif i == (n-1):\n",
    "            #when list of strings are not found in species_XXX, return other\n",
    "            return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(names),len(prices),len(saleprices),len(soldouts),len(stores),len(species)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnames = []\\nprices = []\\nsaleprices = []\\nsoldouts = []\\nstores = []\\nspecies = []\\n\\n#Pearl River Exotics\\nURL = \\'https://www.pearlriverexotics.com\\'\\npage = urlopen(URL)\\nsoup = BeautifulSoup(page,\\'html.parser\\')\\nlinks = soup.findAll(\\'a\\',attrs={\\'class\\':\\'site-nav__link site-nav__link--main\\'})\\nlinks = links[:len(links)-1] #excluding last link \"carnivorous plants for beginners\\'\\n\\nfor link in links:\\n    #Create URL2 to be URL of species type\\n    path = link[\\'href\\']\\n    URL2 = URL + path\\n\\n    page = urlopen(URL2)\\n    soup = BeautifulSoup(page,\\'html.parser\\')\\n    #finds how many pages to loop over\\n    try:\\n        page_count = soup.find(\\'li\\',attrs={\\'class\\':\\'pagination__text\\'}).text.strip()\\n        page_count = page_count[len(page_count)-1:] #grabs how many pages to loop through\\n        page_count = int(page_count)\\n    except (AttributeError,IndexError):\\n        page_count = 1 #only 1 page\\n\\n    for page_number in range(page_count):\\n        #wait random time between 0-5 seconds before scraping data\\n        r = random.randint(0,5)\\n        time.sleep(r)\\n    \\n        page = urlopen(URL2)\\n        soup = BeautifulSoup(page,\\'html.parser\\')\\n        containers = soup.findAll(\\'a\\',attrs={\\'class\\':\\'grid-view-item__link\\'})\\n\\n        for container in containers:\\n            name = container.find(\\'div\\',attrs={\\'class\\':\\'h4 grid-view-item__title\\'}).text.strip()\\n            specie = check_species(name)\\n\\n            price = container.find(\\'span\\',attrs={\\'class\\':\\'product-price__price\\'}).text.strip()\\n            price = price[1:] #remove the 1st character of the string \\'$\\'\\n    \\n            names = names + [name]\\n            prices = prices + [price]\\n            stores = stores + [\\'Pearl River Exotics\\']\\n            saleprices = saleprices + [\\'\\'] #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!change when on sale\\n            species = species + [specie]\\n            \\n            #looks for sold out items\\n            try:\\n                soldout = container.find(\\'span\\',attrs={\\'class\\':\\'product-price__sold-out\\'}).text.strip()\\n                soldouts = soldouts + [soldout.lower()]\\n            except AttributeError:\\n                soldouts = soldouts + [\\'\\']\\n        \\n        #once page is scraped, grabs the URL for next page\\n        try:\\n            path = soup.findAll(\\'a\\',attrs={\\'class\\':\\'btn btn--secondary btn--narrow\\'})\\n            path = path[len(path)-1] #gets the link to the next page\\n            path = path[\\'href\\']\\n            URL2 = URL + path\\n        except (AttributeError,IndexError):\\n            continue #nothing\\n\\nd = {\\'Name\\':names,\\'Price\\':prices,\\'Sale Price\\':saleprices,\\'Sold Out\\':soldouts,\\'Store\\':stores,\\'Species\\':species}\\ndf = pd.DataFrame(d)\\ndf\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "names = []\n",
    "prices = []\n",
    "saleprices = []\n",
    "soldouts = []\n",
    "stores = []\n",
    "species = []\n",
    "\n",
    "#Pearl River Exotics\n",
    "URL = 'https://www.pearlriverexotics.com'\n",
    "page = urlopen(URL)\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "links = soup.findAll('a',attrs={'class':'site-nav__link site-nav__link--main'})\n",
    "links = links[:len(links)-1] #excluding last link \"carnivorous plants for beginners'\n",
    "\n",
    "for link in links:\n",
    "    #Create URL2 to be URL of species type\n",
    "    path = link['href']\n",
    "    URL2 = URL + path\n",
    "\n",
    "    page = urlopen(URL2)\n",
    "    soup = BeautifulSoup(page,'html.parser')\n",
    "    #finds how many pages to loop over\n",
    "    try:\n",
    "        page_count = soup.find('li',attrs={'class':'pagination__text'}).text.strip()\n",
    "        page_count = page_count[len(page_count)-1:] #grabs how many pages to loop through\n",
    "        page_count = int(page_count)\n",
    "    except (AttributeError,IndexError):\n",
    "        page_count = 1 #only 1 page\n",
    "\n",
    "    for page_number in range(page_count):\n",
    "        #wait random time between 0-5 seconds before scraping data\n",
    "        r = random.randint(0,5)\n",
    "        time.sleep(r)\n",
    "    \n",
    "        page = urlopen(URL2)\n",
    "        soup = BeautifulSoup(page,'html.parser')\n",
    "        containers = soup.findAll('a',attrs={'class':'grid-view-item__link'})\n",
    "\n",
    "        for container in containers:\n",
    "            name = container.find('div',attrs={'class':'h4 grid-view-item__title'}).text.strip()\n",
    "            specie = check_species(name)\n",
    "\n",
    "            price = container.find('span',attrs={'class':'product-price__price'}).text.strip()\n",
    "            price = price[1:] #remove the 1st character of the string '$'\n",
    "    \n",
    "            names = names + [name]\n",
    "            prices = prices + [price]\n",
    "            stores = stores + ['Pearl River Exotics']\n",
    "            saleprices = saleprices + [''] #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!change when on sale\n",
    "            species = species + [specie]\n",
    "            \n",
    "            #looks for sold out items\n",
    "            try:\n",
    "                soldout = container.find('span',attrs={'class':'product-price__sold-out'}).text.strip()\n",
    "                soldouts = soldouts + [soldout.lower()]\n",
    "            except AttributeError:\n",
    "                soldouts = soldouts + ['']\n",
    "        \n",
    "        #once page is scraped, grabs the URL for next page\n",
    "        try:\n",
    "            path = soup.findAll('a',attrs={'class':'btn btn--secondary btn--narrow'})\n",
    "            path = path[len(path)-1] #gets the link to the next page\n",
    "            path = path['href']\n",
    "            URL2 = URL + path\n",
    "        except (AttributeError,IndexError):\n",
    "            continue #nothing\n",
    "\n",
    "d = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\n",
    "df = pd.DataFrame(d)\n",
    "df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnames = []\\nprices = []\\nsaleprices = []\\nsoldouts = []\\nstores = []\\nspecies = []\\n\\n#California Carnivores\\nURL = 'https://www.californiacarnivores.com/'\\npage = urlopen(URL)\\nsoup = BeautifulSoup(page,'html.parser')\\nsoup = soup.find('ul',attrs={'class':'sidebar-module__list'})\\nlinks = soup.findAll('a')\\nlinks = [links[i] for i in range(len(links)) if i not in (0,1,3,13,16,22,26,30,31,32,33,34,35,36,37)]\\n#exluding following pages: plant collections (0), easy to grow (1), drosera (3), sarracenia (13),\\n#nepenthes (16), pinguicula (22), utricularia (26), waterwheel, ..., gifts (30-37)\\n\\nfor link in links:\\n    #Create URL2 to be URL of species type\\n    path = link['href']\\n    URL2 = URL + path\\n    \\n    #wait random time between 0-5 seconds before scraping data\\n    r = random.randint(0,5)\\n    time.sleep(r)\\n    \\n    page_next = True\\n    while page_next:\\n        page = urlopen(URL2)\\n        soup = BeautifulSoup(page,'html.parser')\\n        containers = soup.findAll('div',attrs={'class':'grid__item large--one-quarter medium-down--one-half'})\\n        containers_sale = soup.findAll('div',attrs={'class':'grid__item large--one-quarter medium-down--one-half on-sale'})\\n\\n        #grabs information of normal items\\n        for container in containers:\\n            name = container.find('p',attrs={'class':'grid-link__title'}).text.strip()\\n            specie = check_species(name)\\n\\n            price = container.find('p',attrs={'class':'grid-link__meta'}).text.strip()\\n            separate_index = price.find('$') #look for 1st instance of $\\n            price = price[separate_index+2:] #remove the all characters before and including the string '$ '\\n    \\n            names = names + [name]\\n            prices = prices + [price]\\n            saleprices = saleprices + ['']\\n            stores = stores + ['California Carnivores']\\n            soldouts = soldouts + ['']\\n            species = species + [specie]\\n        \\n        #grabs information of sale items\\n        for container in containers_sale:\\n            name = container.find('p',attrs={'class':'grid-link__title'}).text.strip()\\n            specie = check_species(name)\\n\\n            price = container.find('p',attrs={'class':'grid-link__meta'}).text.strip() #price of items on sale should be of form '$ xx.xx\\n$ xx.xx'\\n            separate_index = price.rfind('$') #from end of string, look for 1st instance of $\\n            price_before = price[separate_index:] #gets original price\\n            price_before = price_before[2:] #remove the first 2 characters of the string '$ '\\n            price_after = price[:separate_index] #gets sale price\\n            price_after = price_after[2:] #remove the first 2 characters of the string '$ '\\n            price_after = price_after[:-1] #remove the last 2 characters of the string '$ '\\n    \\n            names = names + [name]\\n            prices = prices + [price_before]\\n            saleprices = saleprices + [price_after]\\n            stores = stores + ['California Carnivores']\\n            soldouts = soldouts + ['']\\n            species = species + [specie]\\n        \\n        #Updates the URL for next page; stops the while loop if there is no next page\\n        try:\\n            path = soup.find('a',attrs={'title':'Next »'})\\n            path = path['href']\\n            URL2 = URL + path\\n        except TypeError:\\n            page_next = False\\n        \\nd = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\\ndf1 = pd.DataFrame(d)\\ndf = df.append(df1,ignore_index=True)\\ndf1\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "names = []\n",
    "prices = []\n",
    "saleprices = []\n",
    "soldouts = []\n",
    "stores = []\n",
    "species = []\n",
    "\n",
    "#California Carnivores\n",
    "URL = 'https://www.californiacarnivores.com/'\n",
    "page = urlopen(URL)\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "soup = soup.find('ul',attrs={'class':'sidebar-module__list'})\n",
    "links = soup.findAll('a')\n",
    "links = [links[i] for i in range(len(links)) if i not in (0,1,3,13,16,22,26,30,31,32,33,34,35,36,37)]\n",
    "#exluding following pages: plant collections (0), easy to grow (1), drosera (3), sarracenia (13),\n",
    "#nepenthes (16), pinguicula (22), utricularia (26), waterwheel, ..., gifts (30-37)\n",
    "\n",
    "for link in links:\n",
    "    #Create URL2 to be URL of species type\n",
    "    path = link['href']\n",
    "    URL2 = URL + path\n",
    "    \n",
    "    #wait random time between 0-5 seconds before scraping data\n",
    "    r = random.randint(0,5)\n",
    "    time.sleep(r)\n",
    "    \n",
    "    page_next = True\n",
    "    while page_next:\n",
    "        page = urlopen(URL2)\n",
    "        soup = BeautifulSoup(page,'html.parser')\n",
    "        containers = soup.findAll('div',attrs={'class':'grid__item large--one-quarter medium-down--one-half'})\n",
    "        containers_sale = soup.findAll('div',attrs={'class':'grid__item large--one-quarter medium-down--one-half on-sale'})\n",
    "\n",
    "        #grabs information of normal items\n",
    "        for container in containers:\n",
    "            name = container.find('p',attrs={'class':'grid-link__title'}).text.strip()\n",
    "            specie = check_species(name)\n",
    "\n",
    "            price = container.find('p',attrs={'class':'grid-link__meta'}).text.strip()\n",
    "            separate_index = price.find('$') #look for 1st instance of $\n",
    "            price = price[separate_index+2:] #remove the all characters before and including the string '$ '\n",
    "    \n",
    "            names = names + [name]\n",
    "            prices = prices + [price]\n",
    "            saleprices = saleprices + ['']\n",
    "            stores = stores + ['California Carnivores']\n",
    "            soldouts = soldouts + ['']\n",
    "            species = species + [specie]\n",
    "        \n",
    "        #grabs information of sale items\n",
    "        for container in containers_sale:\n",
    "            name = container.find('p',attrs={'class':'grid-link__title'}).text.strip()\n",
    "            specie = check_species(name)\n",
    "\n",
    "            price = container.find('p',attrs={'class':'grid-link__meta'}).text.strip() #price of items on sale should be of form '$ xx.xx\\n$ xx.xx'\n",
    "            separate_index = price.rfind('$') #from end of string, look for 1st instance of $\n",
    "            price_before = price[separate_index:] #gets original price\n",
    "            price_before = price_before[2:] #remove the first 2 characters of the string '$ '\n",
    "            price_after = price[:separate_index] #gets sale price\n",
    "            price_after = price_after[2:] #remove the first 2 characters of the string '$ '\n",
    "            price_after = price_after[:-1] #remove the last 2 characters of the string '$ '\n",
    "    \n",
    "            names = names + [name]\n",
    "            prices = prices + [price_before]\n",
    "            saleprices = saleprices + [price_after]\n",
    "            stores = stores + ['California Carnivores']\n",
    "            soldouts = soldouts + ['']\n",
    "            species = species + [specie]\n",
    "        \n",
    "        #Updates the URL for next page; stops the while loop if there is no next page\n",
    "        try:\n",
    "            path = soup.find('a',attrs={'title':'Next »'})\n",
    "            path = path['href']\n",
    "            URL2 = URL + path\n",
    "        except TypeError:\n",
    "            page_next = False\n",
    "        \n",
    "d = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\n",
    "df1 = pd.DataFrame(d)\n",
    "df = df.append(df1,ignore_index=True)\n",
    "df1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnames = []\\nprices = []\\nsaleprices = []\\nsoldouts = []\\nstores = []\\nspecies = []\\n\\n#Cook's Carnivorous Plants\\nURL = 'http://www.flytraps.com/Scripts/'\\npage = urlopen(URL)\\nsoup = BeautifulSoup(page,'html.parser')\\nlinks = soup.findAll('span',attrs={'class':'CPcatDescProd'})\\nlinks = links[:22] #from cobra lily to nepenthes hybrid lowland\\n\\nfor link in links:\\n    #Checks what type of species\\n    kind = link.text\\n    specie = check_species(kind)\\n        \\n    #Create URL2 to be URL of species type\\n    path = link.a['href']\\n    URL2 = URL + path\\n    \\n    #wait random time between 0-5 seconds before scraping data\\n    r = random.randint(0,5)\\n    time.sleep(r)\\n    \\n    page = urlopen(URL2)\\n    soup = BeautifulSoup(page,'html.parser')\\n    containers = soup.findAll('div',attrs={'class':'prod-classic'})\\n    containers_count = len(containers)\\n    \\n    #print(URL2)\\n    #If is not blank, scrapes info. otherwise skips page\\n    if containers_count != 0:\\n        \\n        #each page contains 15 items, goes to next page when page_next mod 15 == 0\\n        page_next = 0\\n        while page_next % 15 == 0:\\n            page = urlopen(URL2)\\n            soup = BeautifulSoup(page,'html.parser')\\n            containers = soup.findAll('div',attrs={'class':'prod-classic'})\\n    \\n            for container in containers:\\n                name = container.h3.text.strip()\\n            \\n                #Most items on site are on sale. Try to process items as if the were on sale. If not on sale, then exception\\n                try:\\n                    price = container.find('del',attrs={'class':'CPprodLPriceV'}).text.strip()\\n                    price = price[1:] #removed '$' at beginning\\n            \\n                    saleprice = container.find('span',attrs={'class':'price'}).text.strip()\\n                    separate_index = saleprice.find('$') #look for 1st instance of $\\n                    saleprice = saleprice[separate_index+1:] #remove the all characters before and including the string '$'\\n                    saleprice = saleprice[:-3] #remove ' ea' from end of sales price\\n                except AttributeError:\\n                    price = container.find('span',attrs={'class':'price'}).text.strip()\\n                    separate_index = price.find('$') #look for 1st instance of $\\n                    price = price[separate_index+1:] #remove the all characters before and including the string '$'\\n                    price = price[:-3] #remove ' ea' from end of sales price\\n                \\n                    saleprice = ''\\n            \\n                names = names + [name]\\n                prices = prices + [price]\\n                stores = stores + ['Cook's Carnivorous Plants']\\n                soldouts = soldouts + ['']\\n                species = species + [specie]\\n                saleprices = saleprices + [saleprice]\\n                page_next = page_next + 1\\n                \\n            #Updates the URL for next page; stops the while loop if there is no next page\\n            if page_next % 15 == 0:\\n                paths = soup.find('div',attrs={'class':'spacing fl-right'})\\n                paths = paths.findAll('a')\\n                path = paths[len(paths)-1]['href'] #href of the last link (should be next page)\\n                URL2 = URL + path\\n        \\nd = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\\ndf1 = pd.DataFrame(d)\\ndf = df.append(df1,ignore_index=True)\\ndf1\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "names = []\n",
    "prices = []\n",
    "saleprices = []\n",
    "soldouts = []\n",
    "stores = []\n",
    "species = []\n",
    "\n",
    "#Cook's Carnivorous Plants\n",
    "URL = 'http://www.flytraps.com/Scripts/'\n",
    "page = urlopen(URL)\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "links = soup.findAll('span',attrs={'class':'CPcatDescProd'})\n",
    "links = links[:22] #from cobra lily to nepenthes hybrid lowland\n",
    "\n",
    "for link in links:\n",
    "    #Checks what type of species\n",
    "    kind = link.text\n",
    "    specie = check_species(kind)\n",
    "        \n",
    "    #Create URL2 to be URL of species type\n",
    "    path = link.a['href']\n",
    "    URL2 = URL + path\n",
    "    \n",
    "    #wait random time between 0-5 seconds before scraping data\n",
    "    r = random.randint(0,5)\n",
    "    time.sleep(r)\n",
    "    \n",
    "    page = urlopen(URL2)\n",
    "    soup = BeautifulSoup(page,'html.parser')\n",
    "    containers = soup.findAll('div',attrs={'class':'prod-classic'})\n",
    "    containers_count = len(containers)\n",
    "    \n",
    "    #print(URL2)\n",
    "    #If is not blank, scrapes info. otherwise skips page\n",
    "    if containers_count != 0:\n",
    "        \n",
    "        #each page contains 15 items, goes to next page when page_next mod 15 == 0\n",
    "        page_next = 0\n",
    "        while page_next % 15 == 0:\n",
    "            page = urlopen(URL2)\n",
    "            soup = BeautifulSoup(page,'html.parser')\n",
    "            containers = soup.findAll('div',attrs={'class':'prod-classic'})\n",
    "    \n",
    "            for container in containers:\n",
    "                name = container.h3.text.strip()\n",
    "            \n",
    "                #Most items on site are on sale. Try to process items as if the were on sale. If not on sale, then exception\n",
    "                try:\n",
    "                    price = container.find('del',attrs={'class':'CPprodLPriceV'}).text.strip()\n",
    "                    price = price[1:] #removed '$' at beginning\n",
    "            \n",
    "                    saleprice = container.find('span',attrs={'class':'price'}).text.strip()\n",
    "                    separate_index = saleprice.find('$') #look for 1st instance of $\n",
    "                    saleprice = saleprice[separate_index+1:] #remove the all characters before and including the string '$'\n",
    "                    saleprice = saleprice[:-3] #remove ' ea' from end of sales price\n",
    "                except AttributeError:\n",
    "                    price = container.find('span',attrs={'class':'price'}).text.strip()\n",
    "                    separate_index = price.find('$') #look for 1st instance of $\n",
    "                    price = price[separate_index+1:] #remove the all characters before and including the string '$'\n",
    "                    price = price[:-3] #remove ' ea' from end of sales price\n",
    "                \n",
    "                    saleprice = ''\n",
    "            \n",
    "                names = names + [name]\n",
    "                prices = prices + [price]\n",
    "                stores = stores + ['Cook\\'s Carnivorous Plants']\n",
    "                soldouts = soldouts + ['']\n",
    "                species = species + [specie]\n",
    "                saleprices = saleprices + [saleprice]\n",
    "                page_next = page_next + 1\n",
    "                \n",
    "            #Updates the URL for next page; stops the while loop if there is no next page\n",
    "            if page_next % 15 == 0:\n",
    "                paths = soup.find('div',attrs={'class':'spacing fl-right'})\n",
    "                paths = paths.findAll('a')\n",
    "                path = paths[len(paths)-1]['href'] #href of the last link (should be next page)\n",
    "                URL2 = URL + path\n",
    "        \n",
    "d = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\n",
    "df1 = pd.DataFrame(d)\n",
    "df = df.append(df1,ignore_index=True)\n",
    "df1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnames = []\\nprices = []\\nsaleprices = []\\nsoldouts = []\\nstores = []\\nspecies = []\\n\\n#Carnivero\\nURL = 'https://www.carnivero.com'\\npage = urlopen(URL)\\nsoup = BeautifulSoup(page,'html.parser')\\nlinks = soup.findAll('div',attrs={'class':'grid__item small--one-half medium-up--one-fifth slide-up-animation animated'})\\nlinks = [links[i] for i in range(10) if i!=1]  #all categories except beginner plants\\n\\n#Looks at plant categories page\\nfor link in links:            \\n    #Create URL2 to be URL of species type\\n    path = link.a['href']\\n    URL2 = URL + path\\n    \\n    #wait random time between 0-5 seconds before scraping data\\n    r = random.randint(0,5)\\n    time.sleep(r)\\n  \\n    page_next = True\\n    while page_next:\\n        page = urlopen(URL2)\\n        soup = BeautifulSoup(page,'html.parser')\\n        containers = soup.findAll('div',attrs={'class':'product grid__item medium-up--one-third small--one-half slide-up-animation animated'})\\n    \\n        for container in containers:\\n            name = container.find('div',attrs={'class':'product__title text-center'}).text.strip()\\n            specie = check_species(name)\\n\\n            try:\\n                price = container.find('span',attrs={'class':'product__price'}).text.strip()\\n                separate_index = price.find('$') #look for 1st instance of $\\n                price = price[separate_index+1:] #remove the all characters before and including the string '$'\\n                prices = prices + [price]\\n                saleprices = saleprices + ['']           \\n            except AttributeError:\\n                #grab original price\\n                price = container.find('s').text.strip()\\n                separate_index = price.find('$') #look for 1st instance of $\\n                price = price[separate_index+1:] #remove the all characters before and including the string '$'\\n                prices = prices + [price]\\n            \\n                #grab new sales price\\n                price = container.find('span',attrs={'class':'product__price--on-sale'}).text.strip()\\n                separate_index = price.find('$') #look for 1st instance of $\\n                price = price[separate_index+1:] #remove the all characters before and including the string '$'\\n                saleprices = saleprices + [price] \\n            \\n            try:\\n                soldout = container.find('strong',attrs={'class':'sold-out-text'}).text.strip()\\n                soldouts = soldouts + [soldout.lower()]\\n            except AttributeError:\\n                soldouts = soldouts + ['']\\n            \\n            names = names + [name]\\n            stores = stores + ['Carnivero']\\n            species = species + [specie]\\n        \\n        #Updates the URL for next page; stops the while loop if there is no next page\\n        try:\\n            path = soup.find('span',attrs={'class':'next'})\\n            path = path.a['href']\\n            URL2 = 'https://www.carnivero.com' + path\\n        except AttributeError:\\n            page_next = False\\n        \\n        \\nd = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\\ndf1 = pd.DataFrame(d)\\ndf = df.append(df1,ignore_index=True)\\ndf1\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "names = []\n",
    "prices = []\n",
    "saleprices = []\n",
    "soldouts = []\n",
    "stores = []\n",
    "species = []\n",
    "\n",
    "#Carnivero\n",
    "URL = 'https://www.carnivero.com'\n",
    "page = urlopen(URL)\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "links = soup.findAll('div',attrs={'class':'grid__item small--one-half medium-up--one-fifth slide-up-animation animated'})\n",
    "links = [links[i] for i in range(10) if i!=1]  #all categories except beginner plants\n",
    "\n",
    "#Looks at plant categories page\n",
    "for link in links:            \n",
    "    #Create URL2 to be URL of species type\n",
    "    path = link.a['href']\n",
    "    URL2 = URL + path\n",
    "    \n",
    "    #wait random time between 0-5 seconds before scraping data\n",
    "    r = random.randint(0,5)\n",
    "    time.sleep(r)\n",
    "  \n",
    "    page_next = True\n",
    "    while page_next:\n",
    "        page = urlopen(URL2)\n",
    "        soup = BeautifulSoup(page,'html.parser')\n",
    "        containers = soup.findAll('div',attrs={'class':'product grid__item medium-up--one-third small--one-half slide-up-animation animated'})\n",
    "    \n",
    "        for container in containers:\n",
    "            name = container.find('div',attrs={'class':'product__title text-center'}).text.strip()\n",
    "            specie = check_species(name)\n",
    "\n",
    "            try:\n",
    "                price = container.find('span',attrs={'class':'product__price'}).text.strip()\n",
    "                separate_index = price.find('$') #look for 1st instance of $\n",
    "                price = price[separate_index+1:] #remove the all characters before and including the string '$'\n",
    "                prices = prices + [price]\n",
    "                saleprices = saleprices + ['']           \n",
    "            except AttributeError:\n",
    "                #grab original price\n",
    "                price = container.find('s').text.strip()\n",
    "                separate_index = price.find('$') #look for 1st instance of $\n",
    "                price = price[separate_index+1:] #remove the all characters before and including the string '$'\n",
    "                prices = prices + [price]\n",
    "            \n",
    "                #grab new sales price\n",
    "                price = container.find('span',attrs={'class':'product__price--on-sale'}).text.strip()\n",
    "                separate_index = price.find('$') #look for 1st instance of $\n",
    "                price = price[separate_index+1:] #remove the all characters before and including the string '$'\n",
    "                saleprices = saleprices + [price] \n",
    "            \n",
    "            try:\n",
    "                soldout = container.find('strong',attrs={'class':'sold-out-text'}).text.strip()\n",
    "                soldouts = soldouts + [soldout.lower()]\n",
    "            except AttributeError:\n",
    "                soldouts = soldouts + ['']\n",
    "            \n",
    "            names = names + [name]\n",
    "            stores = stores + ['Carnivero']\n",
    "            species = species + [specie]\n",
    "        \n",
    "        #Updates the URL for next page; stops the while loop if there is no next page\n",
    "        try:\n",
    "            path = soup.find('span',attrs={'class':'next'})\n",
    "            path = path.a['href']\n",
    "            URL2 = 'https://www.carnivero.com' + path\n",
    "        except AttributeError:\n",
    "            page_next = False\n",
    "        \n",
    "        \n",
    "d = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\n",
    "df1 = pd.DataFrame(d)\n",
    "df = df.append(df1,ignore_index=True)\n",
    "df1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnames = []\\nprices = []\\nsaleprices = []\\nsoldouts = []\\nstores = []\\nspecies = []\\n\\n#Native Exotics\\nURL = 'https://nativeexoticsonline.com'\\nh = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\\nreq = Request(URL,headers=h) \\npage = urlopen(req).read() \\nsoup = BeautifulSoup(page,'html.parser')\\nlinks = soup.find('ul',attrs={'class':'dropdown'})\\nlinks = links.findAll('a')\\npages_of_interest = [5] + list(range(10,15)) #All Neps + Cephs,Drosera,..., Utrics\\nlinks = [links[i] for i in pages_of_interest]\\n\\n#Looks at plant categories page\\nfor link in links:      \\n    #Create URL2 to be URL of species type\\n    path = link['href']\\n    URL2 = URL + path\\n    \\n    #wait random time between 0-5 seconds before scraping data\\n    r = random.randint(0,5)\\n    time.sleep(r)\\n    \\n    page_next = True\\n    while page_next:\\n        req = Request(URL2,headers=h) \\n        page = urlopen(req).read() \\n        soup = BeautifulSoup(page,'html.parser')\\n        container_big = soup.find('ul',attrs={'class':'products columns-4'})\\n        containers = container_big.findAll('li')\\n    \\n        for container in containers:\\n            name = container.find('h2').text.strip()\\n            specie = check_species(name)\\n        \\n            price_list = container.findAll('span',attrs={'class':'woocommerce-Price-amount amount'})\\n            #2 prices listed when on sale. 1st list element is original price, 2nd element is sales price\\n            if len(price_list) == 2:\\n                price = price_list[0].text.strip()\\n                price = price[2:]\\n                saleprice = price_list[1].text.strip()\\n                saleprice = saleprice[2:]             \\n            elif len(price_list) == 1:\\n                price = price_list[0].text.strip()\\n                price = price[2:]\\n                saleprice = ''\\n        \\n            try:\\n                soldout = container.find('span',attrs={'class':'soldout'}).text.strip()\\n                soldouts = soldouts + [soldout.lower()]\\n            except AttributeError:\\n                soldouts = soldouts + ['']\\n            \\n            names = names + [name]\\n            prices = prices + [price]\\n            stores = stores + ['Native Exotics']\\n            saleprices = saleprices + [saleprice]\\n            species = species + [specie]\\n            \\n        #Updates the URL for next page; stops the while loop if there is no next page\\n        try:\\n            URL2 = soup.find('a',attrs={'class':'next page-numbers'})\\n            URL2 = URL2['href']\\n        except TypeError:\\n            page_next = False\\n        \\n        \\nd = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\\ndf1 = pd.DataFrame(d)\\ndf = df.append(df1,ignore_index=True)\\ndf1\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "names = []\n",
    "prices = []\n",
    "saleprices = []\n",
    "soldouts = []\n",
    "stores = []\n",
    "species = []\n",
    "\n",
    "#Native Exotics\n",
    "URL = 'https://nativeexoticsonline.com'\n",
    "h = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "req = Request(URL,headers=h) \n",
    "page = urlopen(req).read() \n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "links = soup.find('ul',attrs={'class':'dropdown'})\n",
    "links = links.findAll('a')\n",
    "pages_of_interest = [5] + list(range(10,15)) #All Neps + Cephs,Drosera,..., Utrics\n",
    "links = [links[i] for i in pages_of_interest]\n",
    "\n",
    "#Looks at plant categories page\n",
    "for link in links:      \n",
    "    #Create URL2 to be URL of species type\n",
    "    path = link['href']\n",
    "    URL2 = URL + path\n",
    "    \n",
    "    #wait random time between 0-5 seconds before scraping data\n",
    "    r = random.randint(0,5)\n",
    "    time.sleep(r)\n",
    "    \n",
    "    page_next = True\n",
    "    while page_next:\n",
    "        req = Request(URL2,headers=h) \n",
    "        page = urlopen(req).read() \n",
    "        soup = BeautifulSoup(page,'html.parser')\n",
    "        container_big = soup.find('ul',attrs={'class':'products columns-4'})\n",
    "        containers = container_big.findAll('li')\n",
    "    \n",
    "        for container in containers:\n",
    "            name = container.find('h2').text.strip()\n",
    "            specie = check_species(name)\n",
    "        \n",
    "            price_list = container.findAll('span',attrs={'class':'woocommerce-Price-amount amount'})\n",
    "            #2 prices listed when on sale. 1st list element is original price, 2nd element is sales price\n",
    "            if len(price_list) == 2:\n",
    "                price = price_list[0].text.strip()\n",
    "                price = price[2:]\n",
    "                saleprice = price_list[1].text.strip()\n",
    "                saleprice = saleprice[2:]             \n",
    "            elif len(price_list) == 1:\n",
    "                price = price_list[0].text.strip()\n",
    "                price = price[2:]\n",
    "                saleprice = ''\n",
    "        \n",
    "            try:\n",
    "                soldout = container.find('span',attrs={'class':'soldout'}).text.strip()\n",
    "                soldouts = soldouts + [soldout.lower()]\n",
    "            except AttributeError:\n",
    "                soldouts = soldouts + ['']\n",
    "            \n",
    "            names = names + [name]\n",
    "            prices = prices + [price]\n",
    "            stores = stores + ['Native Exotics']\n",
    "            saleprices = saleprices + [saleprice]\n",
    "            species = species + [specie]\n",
    "            \n",
    "        #Updates the URL for next page; stops the while loop if there is no next page\n",
    "        try:\n",
    "            URL2 = soup.find('a',attrs={'class':'next page-numbers'})\n",
    "            URL2 = URL2['href']\n",
    "        except TypeError:\n",
    "            page_next = False\n",
    "        \n",
    "        \n",
    "d = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\n",
    "df1 = pd.DataFrame(d)\n",
    "df = df.append(df1,ignore_index=True)\n",
    "df1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnames = []\\nprices = []\\nsaleprices = []\\nsoldouts = []\\nstores = []\\nspecies = []\\n\\n#Carnivorous Plant Nursery\\nURL = 'https://carnivorousplantnursery.com'\\npage = urlopen(URL)\\nsoup = BeautifulSoup(page,'html.parser')\\nlinks = soup.findAll('article')\\nlinks = links[:9] #Only looks at VFT to buterworts\\n\\n#Looks at plant categories page\\nfor link in links:      \\n    #Create URL2 to be URL of species type\\n    path = link.find('figure')\\n    path = path.a['href']\\n    URL2 = URL + path   \\n    \\n    #wait random time between 0-5 seconds before scraping data\\n    r = random.randint(0,5)\\n    time.sleep(r)\\n    \\n    page = urlopen(URL2)\\n    soup = BeautifulSoup(page,'html.parser')\\n    containers = soup.findAll('article',attrs={'class':'productgrid--item imagestyle--cropped-small '})\\n    for container in containers:\\n        name = container.find('h2').text.strip()\\n        specie = check_species(name)\\n    \\n        price = container.find('div',attrs={'class':'price--main'}).text.strip()\\n        separate_index = price.find('$') #look for 1st instance of $\\n        price = price[separate_index+1:] #remove the all characters before and including the string '$'\\n    \\n        names = names + [name]\\n        prices = prices + [price]\\n        stores = stores + ['Carnivorous Plant Nursery']\\n        saleprices = saleprices + [''] #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!change when on sale\\n        species = species + [specie]\\n        \\n        #looks for sold out items\\n        try:\\n            soldout = container.find('span',attrs={'class':'productitem--badge badge--soldout'}).text.strip()\\n            soldouts = soldouts + [soldout.lower()]\\n        except AttributeError:\\n            soldouts = soldouts + ['']\\n            \\n\\nd = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\\ndf1 = pd.DataFrame(d)\\ndf = df.append(df1,ignore_index=True) \\ndf1\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "names = []\n",
    "prices = []\n",
    "saleprices = []\n",
    "soldouts = []\n",
    "stores = []\n",
    "species = []\n",
    "\n",
    "#Carnivorous Plant Nursery\n",
    "URL = 'https://carnivorousplantnursery.com'\n",
    "page = urlopen(URL)\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "links = soup.findAll('article')\n",
    "links = links[:9] #Only looks at VFT to buterworts\n",
    "\n",
    "#Looks at plant categories page\n",
    "for link in links:      \n",
    "    #Create URL2 to be URL of species type\n",
    "    path = link.find('figure')\n",
    "    path = path.a['href']\n",
    "    URL2 = URL + path   \n",
    "    \n",
    "    #wait random time between 0-5 seconds before scraping data\n",
    "    r = random.randint(0,5)\n",
    "    time.sleep(r)\n",
    "    \n",
    "    page = urlopen(URL2)\n",
    "    soup = BeautifulSoup(page,'html.parser')\n",
    "    containers = soup.findAll('article',attrs={'class':'productgrid--item imagestyle--cropped-small '})\n",
    "    for container in containers:\n",
    "        name = container.find('h2').text.strip()\n",
    "        specie = check_species(name)\n",
    "    \n",
    "        price = container.find('div',attrs={'class':'price--main'}).text.strip()\n",
    "        separate_index = price.find('$') #look for 1st instance of $\n",
    "        price = price[separate_index+1:] #remove the all characters before and including the string '$'\n",
    "    \n",
    "        names = names + [name]\n",
    "        prices = prices + [price]\n",
    "        stores = stores + ['Carnivorous Plant Nursery']\n",
    "        saleprices = saleprices + [''] #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!change when on sale\n",
    "        species = species + [specie]\n",
    "        \n",
    "        #looks for sold out items\n",
    "        try:\n",
    "            soldout = container.find('span',attrs={'class':'productitem--badge badge--soldout'}).text.strip()\n",
    "            soldouts = soldouts + [soldout.lower()]\n",
    "        except AttributeError:\n",
    "            soldouts = soldouts + ['']\n",
    "            \n",
    "\n",
    "d = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\n",
    "df1 = pd.DataFrame(d)\n",
    "df = df.append(df1,ignore_index=True) \n",
    "df1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnames = []\\nprices = []\\nsaleprices = []\\nsoldouts = []\\nstores = []\\nspecies = []\\n\\n#Bergen Water Gardens\\n#cannot extract next page button functionality; cannot extract how many pages total\\n#instead keeps going to next page until error\\ntry:\\n    page_number = 1\\n    while page_number < 11:\\n        URL = 'https://bergenwatergardens.com/product-category/carnivorous-plants/?fwp_paged=%d' % page_number\\n        h = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\\n        req = Request(URL,headers=h) \\n        page = urlopen(req).read() \\n        soup = BeautifulSoup(page,'html.parser')\\n    \\n        #wait random time between 0-5 seconds before scraping data\\n        r = random.randint(0,5)\\n        time.sleep(r)\\n        \\n        containers = soup.find('ul',attrs={'class':'products columns-4'})\\n        containers = containers.findAll('li')        \\n        \\n        for container in containers:\\n            name = container.find('h2').text.strip()\\n            specie = check_species(name)\\n\\n            price = container.find('span',attrs={'class':'price'}).text.strip()\\n            #if item is on sale, gets normal price and sale price\\n            try:\\n                container.find('span',attrs={'class':'onsale'}).text.strip() == 'Sale!'\\n    \\n                separate_index = price.find('$') #from BEGINNING of string, look for 1st instance of $\\n                price_before = price[separate_index:] #gets original price\\n                price_before = price_before[1:7] #remove everything except price\\n        \\n                separate_index = price.rfind('$') #from END of string, look for 1st instance of $\\n                price_after = price[separate_index:] #gets original price\\n                price_after = price_after[1:7] #remove everything except price\\n    \\n                price = price_before\\n                saleprice = price_after\\n            #else just get the normal price\\n            except AttributeError:\\n                price = container.find('span',attrs={'class':'price'}).text.strip()\\n                price = price[2:] #removes '$ ' before price\\n                saleprice = ''\\n    \\n            names = names + [name]\\n            prices = prices + [price]\\n            stores = stores + ['Bergen Water Gardens']\\n            saleprices = saleprices + [saleprice]\\n            species = species + [specie]        \\n        \\n            #looks for sold out items\\n            try:\\n                URL2 = container.a['href']\\n                req = Request(URL2,headers=h) \\n                page = urlopen(req).read() \\n                soup = BeautifulSoup(page,'html.parser')\\n                \\n                soldout = soup.find('p',attrs={'class':'stock out-of-stock'}).text.strip()\\n                soldouts = soldouts + [soldout.lower()]\\n            except AttributeError:\\n                soldouts = soldouts + ['']\\n                        \\n        #increase page_number for while loop\\n        page_number = page_number + 1\\n#if container is empty, pass- does nothing, 'pass' in python instead of 'continue'\\nexcept (AttributeError,IndexError):\\n    pass \\n    \\n\\nd = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\\ndf1 = pd.DataFrame(d)\\ndf = df.append(df1,ignore_index=True) \\ndf1\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "names = []\n",
    "prices = []\n",
    "saleprices = []\n",
    "soldouts = []\n",
    "stores = []\n",
    "species = []\n",
    "\n",
    "#Bergen Water Gardens\n",
    "#cannot extract next page button functionality; cannot extract how many pages total\n",
    "#instead keeps going to next page until error\n",
    "try:\n",
    "    page_number = 1\n",
    "    while page_number < 11:\n",
    "        URL = 'https://bergenwatergardens.com/product-category/carnivorous-plants/?fwp_paged=%d' % page_number\n",
    "        h = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "        req = Request(URL,headers=h) \n",
    "        page = urlopen(req).read() \n",
    "        soup = BeautifulSoup(page,'html.parser')\n",
    "    \n",
    "        #wait random time between 0-5 seconds before scraping data\n",
    "        r = random.randint(0,5)\n",
    "        time.sleep(r)\n",
    "        \n",
    "        containers = soup.find('ul',attrs={'class':'products columns-4'})\n",
    "        containers = containers.findAll('li')        \n",
    "        \n",
    "        for container in containers:\n",
    "            name = container.find('h2').text.strip()\n",
    "            specie = check_species(name)\n",
    "\n",
    "            price = container.find('span',attrs={'class':'price'}).text.strip()\n",
    "            #if item is on sale, gets normal price and sale price\n",
    "            try:\n",
    "                container.find('span',attrs={'class':'onsale'}).text.strip() == 'Sale!'\n",
    "    \n",
    "                separate_index = price.find('$') #from BEGINNING of string, look for 1st instance of $\n",
    "                price_before = price[separate_index:] #gets original price\n",
    "                price_before = price_before[1:7] #remove everything except price\n",
    "        \n",
    "                separate_index = price.rfind('$') #from END of string, look for 1st instance of $\n",
    "                price_after = price[separate_index:] #gets original price\n",
    "                price_after = price_after[1:7] #remove everything except price\n",
    "    \n",
    "                price = price_before\n",
    "                saleprice = price_after\n",
    "            #else just get the normal price\n",
    "            except AttributeError:\n",
    "                price = container.find('span',attrs={'class':'price'}).text.strip()\n",
    "                price = price[2:] #removes '$ ' before price\n",
    "                saleprice = ''\n",
    "    \n",
    "            names = names + [name]\n",
    "            prices = prices + [price]\n",
    "            stores = stores + ['Bergen Water Gardens']\n",
    "            saleprices = saleprices + [saleprice]\n",
    "            species = species + [specie]        \n",
    "        \n",
    "            #looks for sold out items\n",
    "            try:\n",
    "                URL2 = container.a['href']\n",
    "                req = Request(URL2,headers=h) \n",
    "                page = urlopen(req).read() \n",
    "                soup = BeautifulSoup(page,'html.parser')\n",
    "                \n",
    "                soldout = soup.find('p',attrs={'class':'stock out-of-stock'}).text.strip()\n",
    "                soldouts = soldouts + [soldout.lower()]\n",
    "            except AttributeError:\n",
    "                soldouts = soldouts + ['']\n",
    "                        \n",
    "        #increase page_number for while loop\n",
    "        page_number = page_number + 1\n",
    "#if container is empty, pass- does nothing, 'pass' in python instead of 'continue'\n",
    "except (AttributeError,IndexError):\n",
    "    pass \n",
    "    \n",
    "\n",
    "d = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\n",
    "df1 = pd.DataFrame(d)\n",
    "df = df.append(df1,ignore_index=True) \n",
    "df1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnames = []\\nprices = []\\nsaleprices = []\\nsoldouts = []\\nstores = []\\nspecies = []\\n\\n#Petflytrap\\nURL = 'https://www.petflytrap.com/'\\npage = urlopen(URL)\\nsoup = BeautifulSoup(page,'html.parser')\\nsoup = soup.find('table')\\nlinks = soup.findAll('td',attrs={'style':'text-indent:45px;'})\\nlinks = [links[i] for i in range(len(links)) if i not in (0,3,10,11,12)]\\n#exluding following pages: All carnivorous plants (0), tropical pitcher plants (3)\\n#which is a duplicate of nepenthes (4), terrariums & kits (10-12)s\\n\\nfor link in links:\\n    #Create URL2 to be URL of species type\\n    path = link.a['href']\\n    URL2 = URL + path\\n    \\n    #wait random time between 0-5 seconds before scraping data\\n    r = random.randint(0,5)\\n    time.sleep(r)\\n    \\n    page_next = True\\n    while page_next:\\n        page = urlopen(URL2)\\n        soup = BeautifulSoup(page,'html.parser')\\n        containers = containers = soup.findAll('td',attrs={'width':'20%'})\\n        \\n        #if there are no items on the page, skips page\\n        if len(containers) == 0:\\n            break\\n        #else grabs information of normal items\\n        else:\\n            for container in containers:\\n                #try processing container info\\n                try:\\n                    name = container.text\\n                    separate_index = name.rfind('Y') #finds the index for 'Y' to remove everything after 'Your Price...'\\n                    separate_index\\n                    name = name[:separate_index] #removes all text after 'Your Price'\\n                    name = name[6:-4] #removes all newlines before and after name\\n                    specie = check_species(name)\\n                    \\n                    price = container.find('td',attrs={'class':'price-info'}).text.strip()\\n                    #if item is on sale, seperate into price_before and price_after\\n                    if 'On sale' in price:\\n                        separate_index = price.find('$') #from BEGINNING of string, look for 1st instance of $\\n                        price_before = price[separate_index:] #gets original price\\n                        price_before = price_before[1:6] #remove everything except price\\n        \\n                        separate_index = price.rfind('$') #from END of string, look for 1st instance of $\\n                        price_after = price[separate_index:] #gets original price\\n                        price_after = price_after[1:6] #remove everything except price\\n                    else:\\n                        separate_index = price.rfind('$') #from end of string, look for 1st instance of $\\n                        price_before = price[separate_index:] #gets original price\\n                        price_before = price_before[1:] #remove the first 2 characters of the string '$ '\\n                        price_after = ''\\n    \\n                    names = names + [name]\\n                    prices = prices + [price_before]\\n                    saleprices = saleprices + [price_after]\\n                    stores = stores + ['Petflytap']\\n                    soldouts = soldouts + ['']\\n                    species = species + [specie]\\n                #if container is empty, pass- does nothing, 'pass' in python instead of 'continue'\\n                except AttributeError:\\n                    pass\\n                \\n            #Updates the URL for next page; stops the while loop if there is no next page\\n            try:\\n                path = soup.findAll('font',attrs={'size':'1'})\\n                path = path[len(path)-1].a['href']\\n                URL2 = URL + path\\n            except (TypeError,IndexError):\\n                page_next = False\\n        \\nd = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\\ndf1 = pd.DataFrame(d)\\ndf = df.append(df1,ignore_index=True)\\ndf1\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ OLD WEBSITE\n",
    "\"\"\"\n",
    "names = []\n",
    "prices = []\n",
    "saleprices = []\n",
    "soldouts = []\n",
    "stores = []\n",
    "species = []\n",
    "\n",
    "#Petflytrap\n",
    "URL = 'https://www.petflytrap.com/'\n",
    "page = urlopen(URL)\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "soup = soup.find('table')\n",
    "links = soup.findAll('td',attrs={'style':'text-indent:45px;'})\n",
    "links = [links[i] for i in range(len(links)) if i not in (0,3,10,11,12)]\n",
    "#exluding following pages: All carnivorous plants (0), tropical pitcher plants (3)\n",
    "#which is a duplicate of nepenthes (4), terrariums & kits (10-12)s\n",
    "\n",
    "for link in links:\n",
    "    #Create URL2 to be URL of species type\n",
    "    path = link.a['href']\n",
    "    URL2 = URL + path\n",
    "    \n",
    "    #wait random time between 0-5 seconds before scraping data\n",
    "    r = random.randint(0,5)\n",
    "    time.sleep(r)\n",
    "    \n",
    "    page_next = True\n",
    "    while page_next:\n",
    "        page = urlopen(URL2)\n",
    "        soup = BeautifulSoup(page,'html.parser')\n",
    "        containers = containers = soup.findAll('td',attrs={'width':'20%'})\n",
    "        \n",
    "        #if there are no items on the page, skips page\n",
    "        if len(containers) == 0:\n",
    "            break\n",
    "        #else grabs information of normal items\n",
    "        else:\n",
    "            for container in containers:\n",
    "                #try processing container info\n",
    "                try:\n",
    "                    name = container.text\n",
    "                    separate_index = name.rfind('Y') #finds the index for 'Y' to remove everything after 'Your Price...'\n",
    "                    separate_index\n",
    "                    name = name[:separate_index] #removes all text after 'Your Price'\n",
    "                    name = name[6:-4] #removes all newlines before and after name\n",
    "                    specie = check_species(name)\n",
    "                    \n",
    "                    price = container.find('td',attrs={'class':'price-info'}).text.strip()\n",
    "                    #if item is on sale, seperate into price_before and price_after\n",
    "                    if 'On sale' in price:\n",
    "                        separate_index = price.find('$') #from BEGINNING of string, look for 1st instance of $\n",
    "                        price_before = price[separate_index:] #gets original price\n",
    "                        price_before = price_before[1:6] #remove everything except price\n",
    "        \n",
    "                        separate_index = price.rfind('$') #from END of string, look for 1st instance of $\n",
    "                        price_after = price[separate_index:] #gets original price\n",
    "                        price_after = price_after[1:6] #remove everything except price\n",
    "                    else:\n",
    "                        separate_index = price.rfind('$') #from end of string, look for 1st instance of $\n",
    "                        price_before = price[separate_index:] #gets original price\n",
    "                        price_before = price_before[1:] #remove the first 2 characters of the string '$ '\n",
    "                        price_after = ''\n",
    "    \n",
    "                    names = names + [name]\n",
    "                    prices = prices + [price_before]\n",
    "                    saleprices = saleprices + [price_after]\n",
    "                    stores = stores + ['Petflytap']\n",
    "                    soldouts = soldouts + ['']\n",
    "                    species = species + [specie]\n",
    "                #if container is empty, pass- does nothing, 'pass' in python instead of 'continue'\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                \n",
    "            #Updates the URL for next page; stops the while loop if there is no next page\n",
    "            try:\n",
    "                path = soup.findAll('font',attrs={'size':'1'})\n",
    "                path = path[len(path)-1].a['href']\n",
    "                URL2 = URL + path\n",
    "            except (TypeError,IndexError):\n",
    "                page_next = False\n",
    "        \n",
    "d = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\n",
    "df1 = pd.DataFrame(d)\n",
    "df = df.append(df1,ignore_index=True)\n",
    "df1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Sale Price</th>\n",
       "      <th>Sold Out</th>\n",
       "      <th>Store</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brocchinia acuminata bromeliad - Small Potted</td>\n",
       "      <td>19.99</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Petflytap</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Name  Price Sale Price Sold Out  \\\n",
       "0  Brocchinia acuminata bromeliad - Small Potted  19.99                       \n",
       "\n",
       "       Store Species  \n",
       "0  Petflytap   other  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ ERROR, NEED TO FIX\n",
    "names = []\n",
    "prices = []\n",
    "saleprices = []\n",
    "soldouts = []\n",
    "stores = []\n",
    "species = []\n",
    "\n",
    "#Petflytrap\n",
    "URL = 'https://www.petflytrap.com/All-carnivorous-plants_c_11.html'\n",
    "#URL = 'https://www.petflytrap.com/All-carnivorous-plants_c_11-1-3.html'\n",
    "h = {'User-Agent':'Mozilla/5.0'}\n",
    "req = Request(URL,headers=h) \n",
    "page = urlopen(req).read()\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "\n",
    "containers = soup.find('div',attrs={'class':'product-items product-items-4'})\n",
    "\n",
    "name = containers.findAll('div',attrs={'class':'name'})[0].text.strip()\n",
    "specie = check_species(name)\n",
    "price = containers.findAll('div',attrs={'class':'price'})[0].text.strip()\n",
    "price = price[1:] #removes '$' before price\n",
    "\n",
    "\n",
    "\n",
    "names = names + [name]\n",
    "prices = prices + [price]\n",
    "saleprices = saleprices + ['']\n",
    "stores = stores + ['Petflytap']\n",
    "soldouts = soldouts + ['']\n",
    "species = species + [specie]\n",
    "\n",
    "        \n",
    "d = {'Name':names,'Price':prices,'Sale Price':saleprices,'Sold Out':soldouts,'Store':stores,'Species':species}\n",
    "df1 = pd.DataFrame(d)\n",
    "#df = df.append(df1,ignore_index=True)\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brocchinia acuminata bromeliad - Small Potted'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup\n",
    "containers = soup.find('div',attrs={'class':'product-items product-items-4'})\n",
    "#containers = soup.findAll('div',attrs={'class':'price'})\n",
    "containers = containers.findAll('div',attrs={'class':'name'})\n",
    "containers[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brocchinia acuminata bromeliad - Small Potted'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_species(name)\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"paging-links\" href=\"All-carnivorous-plants_c_11-2-3.html\">2</a>,\n",
       " <a class=\"paging-links\" href=\"All-carnivorous-plants_c_11-3-3.html\">3</a>,\n",
       " <a class=\"paging-links\" href=\"All-carnivorous-plants_c_11-4-3.html\">4</a>,\n",
       " <a class=\"paging-links\" href=\"All-carnivorous-plants_c_11-5-3.html\">5</a>,\n",
       " <a class=\"paging-links\" href=\"All-carnivorous-plants_c_11-6-3.html\">6</a>,\n",
       " <a class=\"paging-links\" href=\"All-carnivorous-plants_c_11-7-3.html\">7</a>,\n",
       " <a class=\"paging-links\" href=\"All-carnivorous-plants_c_11-15-3.html\">15</a>,\n",
       " <a href=\"All-carnivorous-plants_c_11-2-3.html\">Next Page</a>,\n",
       " <a class=\"category-viewall\" href=\"All-carnivorous-plants_c_11.html?viewall=1\">View All</a>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div',attrs={'class':'paging'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Predatory Plants\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Predatory Plants\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-37c9bed1af5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#Converting exported df back into correct format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#removing column of indecies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3697\u001b[1;33m                                            errors=errors)\n\u001b[0m\u001b[0;32m   3698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3143\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   4402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 4404\u001b[1;33m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[0;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Unnamed: 0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#Export concatinated df to Excel\n",
    "\n",
    "date = '2020_02_05' #date format 2020_01_01\n",
    "file_name_1 = 'df_%s.xlsx' % date\n",
    "#df.to_excel(file_name_1)\n",
    "\n",
    "\n",
    "#Converting exported df back into correct format\n",
    "df = pd.read_excel(file_name_1)\n",
    "df = df.drop(columns='Unnamed: 0') #removing column of indecies\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once the complete database is compiled, apply tagging function - tags any species found in product\n",
    "#https://www.carnivorousplants.co.uk/resources/nepenthes-interactive-guide/\n",
    "#https://en.wikipedia.org/wiki/List_of_Nepenthes_species\n",
    "#https://en.wikipedia.org/wiki/List_of_Utricularia_species\n",
    "\n",
    "#compare product to database of species known\n",
    "#if str in database, then species = str\n",
    "df['Cross1'] = ''\n",
    "df['Cross2'] = ''\n",
    "df['Cross3'] = ''\n",
    "df['Cross4'] = ''\n",
    "df['Cross5'] = ''\n",
    "df['Cross6'] = ''\n",
    "df['Cross7'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 175\n",
    "name_nepenthes = [''] * n\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_Nepenthes_species'\n",
    "page = urlopen(URL)\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "containers = soup.findAll('tr')\n",
    "\n",
    "for i in range(1,n):\n",
    "    container = containers[i]\n",
    "    name = container.find('a')\n",
    "    name_nepenthes[i] = name['title'][10:] #removing 'Nepenthes ' from string\n",
    "    \n",
    "name_nepenthes = name_nepenthes[1:] #remove header row\n",
    "\n",
    "#lady = lady luck, bill = bill bailey\n",
    "name_nepenthes_hybrids = ['lady','hookeriana','suki','bill','diana','briggsiana','rokko','ventrata', \\\n",
    "                          'miranda','trusmadiensis']\n",
    "name_nepenthes_hybrids_cross = [['ventricosa','ampullaria'],['ampullaria','rafflesiana'], \\\n",
    "                                ['rafflesiana','sibuyanensis'],['ventricosa','singalana'], \\\n",
    "                                ['ventricosa','sibuyanensis','ampullaria'],['ventricosa','lowii'], \\\n",
    "                                ['maxima','thorelii'],['ventricosa','alata'],['maxima','northiana'], \\\n",
    "                                ['lowii','macrophylla']]\n",
    "\n",
    "name_nepenthes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_utricularia = []\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_Utricularia_species'\n",
    "page = urlopen(URL)\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "containers = soup.findAll('ul')\n",
    "\n",
    "#scraping species names from containers[4] to containers[39]\n",
    "for i in range(4,40):\n",
    "    container = containers[i]\n",
    "    string_names = container.text.replace('Utricularia ','').split('\\n') #removing 'Utricularia ' from string\n",
    "\n",
    "    #Cleaning up Cross names\n",
    "    for j in range(len(string_names)):\n",
    "        #Removing Footnotes [1]\n",
    "        if '[' in string_names[j]:\n",
    "            index = string_names[j].find('[')\n",
    "            string_names[j] = string_names[j][:(index-1)]\n",
    "\n",
    "            \n",
    "    name_utricularia = name_utricularia + string_names\n",
    "\n",
    "    \n",
    "##have not found a natural hybrid yet; no need for this section yet\n",
    "#name_utricularia_hybrids = ['lady','hookeriana','suki','bill','diana','briggsiana','rokko','ventrata', \\\n",
    "#                          'miranda','trusmadiensis']\n",
    "#name_utricularia_hybrids_cross = [['ventricosa','ampullaria'],['ampullaria','rafflesiana'], \\\n",
    "#                                ['rafflesiana','sibuyanensis'],['ventricosa','singalana'], \\\n",
    "#                                ['ventricosa','sibuyanensis','ampullaria'],['ventricosa','lowii'], \\\n",
    "#                                ['maxima','thorelii'],['ventricosa','alata'],['maxima','northiana'], \\\n",
    "#                                ['lowii','macrophylla']]\n",
    "\n",
    "name_utricularia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_pinguicula = []\n",
    "URL = 'https://en.wikipedia.org/wiki/List_of_Pinguicula_species'\n",
    "page = urlopen(URL)\n",
    "soup = BeautifulSoup(page,'html.parser')\n",
    "containers = soup.findAll('ul')\n",
    "\n",
    "\n",
    "#scraping species names from containers[4] to containers[26]\n",
    "for i in range(4,27):\n",
    "    container = containers[i]\n",
    "    string_names = container.text.replace('Pinguicula ','').split('\\n') #removing 'Pinguicula ' from string\n",
    "    \n",
    "    #Cleaning up Cross names\n",
    "    for j in range(len(string_names)):\n",
    "        #Removing text after -\n",
    "        if '–' in string_names[j]:\n",
    "            index = string_names[j].find('–')\n",
    "            string_names[j] = string_names[j][:(index-1)]\n",
    "        #Removing Footnotes [1]\n",
    "        elif '[' in string_names[j]:\n",
    "            index = string_names[j].find('[')\n",
    "            string_names[j] = string_names[j][:(index-1)]\n",
    "            \n",
    "            \n",
    "    name_pinguicula = name_pinguicula + string_names\n",
    "    \n",
    "\n",
    "#john = john rizzi, alfred = alfred lau, \n",
    "name_pinguicula_hybrids = ['sethos','weser','john','nivalis','alfred','florian']\n",
    "name_pinguicula_hybrids_cross = [['ehlersiae','moranensis'],['moranensis','ehlersiae'], \\\n",
    "                                ['moranensis','?'],['nivalis'],['gigantea','?'],['debbertiana','esseriana']]\n",
    "\n",
    "name_pinguicula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i is row index, j is column index\n",
    "#len(df) gives number of rows in df\n",
    "for i in range(len(df)):\n",
    "    #removing all of the special characters:()[] so the string is split into 'ventricosa' and not '(ventricosa''\n",
    "    string_split = df.at[i,'Name'].replace('(','').replace(')','').replace('[','').replace(']','').replace(',','')\n",
    "    string_split = string_split.replace('-','').replace('–','').replace('\\'','').replace('\\\"','').replace('‘','')\n",
    "    string_split = string_split.replace('’','')\n",
    "    #convert entire string to lowercase\n",
    "    string_split = string_split.lower()\n",
    "    \n",
    "    #Filling in Cross columns\n",
    "    string_split = string_split.split(' ')\n",
    "    count = 1 #counter for column name\n",
    "    repeat_cross = []\n",
    "    \n",
    "    #If-Else If by species\n",
    "    if df.at[i,'Species'] == 'nepenthes':\n",
    "        for j in range(len(string_split)):\n",
    "            #if cross is in name_nepenthes and not repeated, add it to CrossX column\n",
    "            if (string_split[j] in name_nepenthes) and (string_split[j] not in repeat_cross):\n",
    "                column = 'Cross%d' % count\n",
    "                count = count + 1\n",
    "                df.loc[[i],[column]] = string_split[j]\n",
    "                repeat_cross = repeat_cross + [string_split[j]]\n",
    "            #if cross is in name_nepenthes_hybrid and not repeated, add it to CrossX column\n",
    "            elif (string_split[j] in name_nepenthes_hybrids) and (string_split[j] not in repeat_cross):\n",
    "                m = name_nepenthes_hybrids.index(string_split[j])\n",
    "                for k in range(len(name_nepenthes_hybrids_cross[m])):\n",
    "                    column = 'Cross%d' % count\n",
    "                    count = count + 1\n",
    "                    df.loc[[i],[column]] = name_nepenthes_hybrids_cross[m][k]\n",
    "                    repeat_cross = repeat_cross + [string_split[j]]\n",
    "                    repeat_cross = repeat_cross + [name_nepenthes_hybrids_cross[m][k]]\n",
    "    elif df.at[i,'Species'] == 'utricularia':\n",
    "        for j in range(len(string_split)):\n",
    "            #if cross is in name_utricularia and not repeated, add it to CrossX column\n",
    "            if (string_split[j] in name_utricularia) and (string_split[j] not in repeat_cross):\n",
    "                column = 'Cross%d' % count\n",
    "                count = count + 1\n",
    "                df.loc[[i],[column]] = string_split[j]\n",
    "                repeat_cross = repeat_cross + [string_split[j]]  \n",
    "    elif df.at[i,'Species'] == 'pinguicula':\n",
    "        for j in range(len(string_split)):\n",
    "            #if cross is in name_pinguicula and not repeated, add it to CrossX column\n",
    "            if (string_split[j] in name_pinguicula) and (string_split[j] not in repeat_cross):\n",
    "                column = 'Cross%d' % count\n",
    "                count = count + 1\n",
    "                df.loc[[i],[column]] = string_split[j]\n",
    "                repeat_cross = repeat_cross + [string_split[j]]\n",
    "            #if cross is in name_pinguicula_hybrid and not repeated, add it to CrossX column\n",
    "            elif (string_split[j] in name_pinguicula_hybrids) and (string_split[j] not in repeat_cross):\n",
    "                m = name_pinguicula_hybrids.index(string_split[j])\n",
    "                for k in range(len(name_pinguicula_hybrids_cross[m])):\n",
    "                    column = 'Cross%d' % count\n",
    "                    count = count + 1\n",
    "                    df.loc[[i],[column]] = name_pinguicula_hybrids_cross[m][k]\n",
    "                    repeat_cross = repeat_cross + [string_split[j]]\n",
    "                    repeat_cross = repeat_cross + [name_pinguicula_hybrids_cross[m][k]]\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "#create special if clause when tag is not found, open product page and scan description text for tags\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable               Type             Data/Info\n",
      "-------------------------------------------------\n",
      "BeautifulSoup          type             <class 'bs4.BeautifulSoup'>\n",
      "Request                type             <class 'urllib.request.Request'>\n",
      "URL                    str              https://www.petflytrap.com/\n",
      "check_species          function         <function check_species at 0x0000027C440A9598>\n",
      "containers             Tag              <div class=\"product-items<...>>\\n</div>\\n</div>\\n</div>\n",
      "date                   str              2020_02_05\n",
      "df                     DataFrame                                 <...>\\n[2136 rows x 6 columns]\n",
      "file_name_1            str              df_2020_02_05.xlsx\n",
      "h                      dict             n=1\n",
      "names                  list             n=0\n",
      "np                     module           <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "page                   bytes            b'<!doctype html>\\n<html <...>/script></body>\\n</html>'\n",
      "pd                     module           <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "prices                 list             n=0\n",
      "random                 module           <module 'random' from 'C:<...>aconda3\\\\lib\\\\random.py'>\n",
      "req                    Request          <urllib.request.Request o<...>ct at 0x0000027C440BAEF0>\n",
      "saleprices             list             n=0\n",
      "soldouts               list             n=0\n",
      "soup                   BeautifulSoup    <!DOCTYPE doctype html>\\n<...></script></body>\\n</html>\n",
      "species                list             n=0\n",
      "species_all            list             n=27\n",
      "species_cephs          list             n=2\n",
      "species_darlingtonia   list             n=4\n",
      "species_heli           list             n=2\n",
      "species_neps           list             n=3\n",
      "species_pings          list             n=3\n",
      "species_sarracenia     list             n=2\n",
      "species_sundew         list             n=3\n",
      "species_utric          list             n=3\n",
      "species_vft            list             n=5\n",
      "stores                 list             n=0\n",
      "time                   module           <module 'time' (built-in)>\n",
      "urlopen                function         <function urlopen at 0x0000027C43FB49D8>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Check what does not have any Cross1 tag\n",
    "for i in range(len(df)):\n",
    "    if (df.loc[i,'Cross1'] is '') and (df.loc[i,'Species'] == 'utricularia'):\n",
    "        print(i,df.loc[i,'Name'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Export df with Cross1, ... ,Cross6 to Excel\n",
    "file_name_2 = 'dfCross_%s.xlsx' % date\n",
    "df.to_excel(file_name_2)\n",
    "\"\"\"\n",
    "\n",
    "#Converting exported df back into correct format\n",
    "df = pd.read_excel('dfCross_2020_02_05.xlsx')\n",
    "df = df.drop(columns='Unnamed: 0') #removing column of indecies\n",
    "\n",
    "df\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
